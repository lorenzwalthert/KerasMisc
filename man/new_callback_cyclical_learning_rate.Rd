% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/core.R
\name{new_callback_cyclical_learning_rate}
\alias{new_callback_cyclical_learning_rate}
\title{Initiate a new cyclical learning rate scheduler}
\usage{
new_callback_cyclical_learning_rate(
  base_lr = 0.001,
  max_lr = 0.006,
  step_size = 2000,
  mode = "triangular",
  gamma = 1,
  scale_fn = NULL,
  scale_mode = "cycle",
  patience = Inf,
  factor = 0.9,
  decrease_base_lr = TRUE,
  cooldown = 2,
  verbose = 1
)
}
\arguments{
\item{base_lr}{Initial learning rate which is the lower boundary in the
cycle.}

\item{max_lr}{Upper boundary in the cycle. Functionally, it defines the
cycle amplitude (\code{max_lr - base_lr}). The learning rate at any cycle is
the sum of \code{base_lr} and some scaling of the amplitude; therefore \code{max_lr}
may not actually be reached depending on scaling function.}

\item{step_size}{Number of training iterations per half cycle. Authors
suggest setting step_size \code{2-8} x training iterations in epoch.}

\item{mode}{One of "triangular", "triangular2" or "exp_range". Default
"triangular". Values correspond to policies detailed above. If \code{scale_fn}
is not \code{NULL}, this argument is ignored.}

\item{gamma}{Constant in \code{exp_range} scaling function: \verb{gamma^(cycle iterations)}}

\item{scale_fn}{Custom scaling policy defined by a single argument anonymous
function, where \verb{0 <= scale_fn(x) <= 1} for all \code{x >= 0}. Mode paramater is
ignored.}

\item{scale_mode}{Either "cycle" or "iterations". Defines whether \code{scale_fn}
is evaluated on cycle number or cycle iterations (training iterations since
start of cycle). Default is "cycle".}

\item{patience}{The number of epochs of training without validation loss
improvement that the callback will wait before it adjusts \code{base_lr} and
\code{max_lr}. Requires a \code{validation_data} to be passed in the \code{\link[keras:reexports]{keras::fit()}}
call if set to something else than \code{Inf}.}

\item{factor}{An numeric vector of lenght one which will scale \code{max_lr} and
(if applicable according to \code{decrease_base_lr}) \code{base_lr}
after \code{patience} epochs without improvement in the validation loss.}

\item{decrease_base_lr}{Boolean indicating whether \code{base_lr} should also be
scaled with \code{factor} or not.}

\item{cooldown}{Number of epochs to wait before resuming normal operation
after learning rate has been reduced.}

\item{verbose}{Currently supporting 0 (silent) and 1 (verbose).}
}
\description{
This callback implements a cyclical learning rate policy (CLR).
The method cycles the learning rate between two boundaries with
some constant frequency, as detailed in
\href{https://arxiv.org/abs/1506.01186}{this paper}. In addition, the
call-back supports scaled learning-rate bandwidths (see section
'Differences to the Python implementation'). Note that this callback is
very general as it can be used to specify:
}
\details{
\itemize{
\item constant learning rates.
\item cyclical learning rates.
\item decayling learning rates depending on validation loss such as
\code{\link[keras:callback_reduce_lr_on_plateau]{keras::callback_reduce_lr_on_plateau()}}
\item learning rates with scaled bandwidths.
Apart from this, the
implementation follows the
\href{https://github.com/bckenstler/CLR}{Python implementation} quite closey.
}

The amplitude of the cycle can be scaled on a per-iteration or per-cycle
basis. This class has three built-in policies, as put forth in the paper.
\itemize{
\item "triangular": A basic triangular cycle w/ no amplitude scaling.
\item "triangular2": A basic triangular cycle that scales initial amplitude by
half each cycle.
\item "exp_range": A cycle that scales initial amplitude by gamma**(cycle
iterations) at each cycle iteration.
}

For more details, please see paper.
}
\section{Differences to Python implementation}{

This implementation differs from the
\href{https://github.com/bckenstler/CLR}{Python implementation} in the following
aspects:
\itemize{
\item \emph{scaled learning-rate bandwidth on plateau} is supported. Via the
arguments \code{patience}, \code{factor} and \code{decrease_base_lr}, the user has
control over if and when the boundaries of the learning rate are adjusted.
This feature allows to combine decaying learning rates with cyclical
learning rates. Typically, one wants to reduce the learning rate bandwith
after validation loss has stopped improving for some time.
Note that both \code{factor < 1} and \code{patience < Inf} must hold
in order for this feature to take effect.
\item The \code{history} dataframe in the return value of this callback has a column
\code{epochs} in addition to \code{itterations} and \code{lr}.
\item The callback returns a \code{history_epoch} dataframe that just contains the
epochs and the learning rates at the end of the epoch. This is less
granular than the \code{history} element.
\item All column names in \code{history} and \code{history_epoch} are - opposed to the
Python implementation - in singular.
}
}

\examples{
library(keras)
dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) \%<-\% dataset

mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)


model <- keras_model_sequential() \%>\%
  layer_dense(
    units = 64, activation = "relu",
    input_shape = dim(train_data)[[2]]
  ) \%>\%
  layer_dense(units = 64, activation = "relu") \%>\%
  layer_dense(units = 1)
model \%>\% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = "mse",
  metrics = c("mae")
)

callback_clr <- new_callback_cyclical_learning_rate(
  step_size = 32,
  base_lr = 0.001,
  max_lr = 0.006,
  gamma = 0.99,
  mode = "exp_range"
)
model \%>\% fit(
  train_data, train_targets,
  validation_data = list(test_data, test_targets),
  epochs = 10, verbose = 1,
  callbacks = list(callback_clr)
)
callback_clr$history
plot_clr_history(callback_clr, backend = "base")
}
\concept{callbacks}
